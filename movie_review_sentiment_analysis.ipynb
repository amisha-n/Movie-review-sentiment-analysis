{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "movie-review-sentiment-analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsM9ChR5WoHC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "feedf996-d209-4a20-c3bf-8f0b5a5ff6a3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcagdinvWzu4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cb1c937-af20-47dd-c0d3-9bccf7aadf59"
      },
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlTJnQV0mTW_"
      },
      "source": [
        "train=pd.read_csv('/content/drive/MyDrive/Project data/Movie review sentiment analysis/labeledTrainData.tsv', delimiter=\"\\t\")\n",
        "test=pd.read_csv('/content/drive/MyDrive/Project data/Movie review sentiment analysis/testData.tsv', delimiter=\"\\t\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "3AOJIAEMmTXA",
        "outputId": "a3f6ab9d-9e2c-4617-fbff-d0d7aca43f96"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5814_8</td>\n",
              "      <td>1</td>\n",
              "      <td>With all this stuff going down at the moment w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2381_9</td>\n",
              "      <td>1</td>\n",
              "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7759_3</td>\n",
              "      <td>0</td>\n",
              "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3630_4</td>\n",
              "      <td>0</td>\n",
              "      <td>It must be assumed that those who praised this...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9495_8</td>\n",
              "      <td>1</td>\n",
              "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id  sentiment                                             review\n",
              "0  5814_8          1  With all this stuff going down at the moment w...\n",
              "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
              "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
              "3  3630_4          0  It must be assumed that those who praised this...\n",
              "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "476RHeCnmTXB"
      },
      "source": [
        "Data Cleaning and text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96KdF3qQmTXB"
      },
      "source": [
        "def preprocessor(review):\n",
        "    #Removing HTML markup\n",
        "    review_text = BeautifulSoup(review).get_text()\n",
        "    \n",
        "    #Remove non-letters using re\n",
        "    review_text = re.sub('[^a-zA-Z]', \" \", review_text)\n",
        "    \n",
        "    review_text = review_text.lower()\n",
        "    review_text = review_text.split() #Tokenize\n",
        "    \n",
        "    #Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    words = [stemmer.stem(word) for word in review_text if word not in stopwords.words('english')]\n",
        "    output = ' '.join(words)\n",
        "    \n",
        "    return output\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBfO8T1hmTXC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "4f146418-a6af-4769-88c3-4723ec74966f"
      },
      "source": [
        "corpus = []\n",
        "for i in range(len(train)):\n",
        "    corpus.append(preprocessor(train['review'][i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-637c2159955a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-4fa690e9ecf8>\u001b[0m in \u001b[0;36mpreprocessor\u001b[0;34m(review)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#Stemming\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreview_text\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-4fa690e9ecf8>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#Stemming\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreview_text\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \"\"\"\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         return [line for line in line_tokenize(self.raw(fileids))\n\u001b[0m\u001b[1;32m     23\u001b[0m                 if not line.startswith(ignore_lines_startswith)]\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mraw\u001b[0;34m(self, fileids)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg_TiV6LmTXD"
      },
      "source": [
        "Using Bag of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McVlRmHNmTXD"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(max_features = 5000) \n",
        "X_bow = vectorizer.fit_transform(corpus)\n",
        "\n",
        "X_bow = X_bow.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfWlAAmqznpl",
        "outputId": "3a2ab8d8-d769-4f20-f1da-0a4349043248"
      },
      "source": [
        "X_bow.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 5000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZDLzvJFztUb"
      },
      "source": [
        "y = train['sentiment']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f55z9hbEw5HL"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_bowtrain, X_bowtest, y_train, y_test = train_test_split(X_bow, y, test_size=0.30, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkepFAbr0ajS"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "forest = RandomForestClassifier(n_estimators = 100)\n",
        "forest = forest.fit(X_bowtrain, y_train )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BO52z5w40_7M"
      },
      "source": [
        "y_pred = forest.predict(X_bowtest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBxj5Do01GR6",
        "outputId": "77f1de7b-1be0-4ea5-e851-c985ed72f61b"
      },
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.classification_report(y_test, y_pred))\n",
        "print(metrics.confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85      3738\n",
            "           1       0.85      0.84      0.85      3762\n",
            "\n",
            "    accuracy                           0.85      7500\n",
            "   macro avg       0.85      0.85      0.85      7500\n",
            "weighted avg       0.85      0.85      0.85      7500\n",
            "\n",
            "[[3201  537]\n",
            " [ 602 3160]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVSJBYLs1Y6l"
      },
      "source": [
        "Using Keras Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Eo_Et2u1luu"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlAznfiV1l0_"
      },
      "source": [
        "### Vocabulary size\n",
        "voc_size=10000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "0xWa-aTt145V",
        "outputId": "ae7dfd15-d409-4235-fd6c-f97f22c7c873"
      },
      "source": [
        "corpus[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'classic war world timothi hine entertain film obvious goe great effort length faith recreat h g well classic book mr hine succe watch film appreci fact standard predict hollywood fare come everi year e g spielberg version tom cruis slightest resembl book obvious everyon look differ thing movi envis amateur critic look critic everyth other rate movi import base like entertain peopl never agre critic enjoy effort mr hine put faith h g well classic novel found entertain made easi overlook critic perceiv shortcom'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-9f0Gtb1l7P"
      },
      "source": [
        "#One-hot encoding\n",
        "onehot_repr=[one_hot(words,voc_size)for words in corpus] \n",
        "#onehot_repr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2Ia85zh2PJK",
        "outputId": "85d53d9d-bb29-4a16-c52b-b6e3f14cbd81"
      },
      "source": [
        "len(corpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FILYiscR2G94",
        "outputId": "30f37041-1213-4216-963a-a2bdc7fd271a"
      },
      "source": [
        "len(max(onehot_repr, key=len))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1416"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa7AVM5m366j",
        "outputId": "c56c5cb2-a95e-4308-fa1c-9c5919e00b5f"
      },
      "source": [
        "sent_length = 200\n",
        "embedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\n",
        "print(embedded_docs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[9879 3724 8513 ... 4378 2674 4914]\n",
            " [   0    0    0 ... 6799 3648 6528]\n",
            " [6410 4477 1794 ... 7219 4594 6958]\n",
            " ...\n",
            " [   0    0    0 ... 8513 7219 9906]\n",
            " [   0    0    0 ...    5 1714 3778]\n",
            " [   0    0    0 ... 3543 2058 6065]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJVpIjfN48Pa"
      },
      "source": [
        "\n",
        "X_final=np.array(embedded_docs)\n",
        "y_final=np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0Oe_eFl5P0v"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.30, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbGQhg3Y4k9c",
        "outputId": "3dbb96f3-5963-489b-af14-273d1c0a9e25"
      },
      "source": [
        "embedding_vector_features=100\n",
        "model=Sequential()\n",
        "model.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 200, 100)          1000000   \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 1,080,501\n",
            "Trainable params: 1,080,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmEAYjSv5VFb"
      },
      "source": [
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    min_delta=0.01, # minimium amount of change to count as an improvement\n",
        "    patience=10, # how many epochs to wait before stopping\n",
        "    restore_best_weights=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a0aKsQp5VKg",
        "outputId": "e708e52a-f41d-4245-8c19-8fef4647782c"
      },
      "source": [
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    batch_size=1024,\n",
        "    epochs=100,\n",
        "    callbacks=[early_stopping], # put your callbacks in a list\n",
        "    verbose=2,  # turn off training log\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "18/18 - 4s - loss: 0.6900 - accuracy: 0.6375 - val_loss: 0.6215 - val_accuracy: 0.6552\n",
            "Epoch 2/100\n",
            "18/18 - 2s - loss: 0.5670 - accuracy: 0.7636 - val_loss: 0.4837 - val_accuracy: 0.8003\n",
            "Epoch 3/100\n",
            "18/18 - 2s - loss: 0.3683 - accuracy: 0.8544 - val_loss: 0.3500 - val_accuracy: 0.8573\n",
            "Epoch 4/100\n",
            "18/18 - 2s - loss: 0.2668 - accuracy: 0.8967 - val_loss: 0.3190 - val_accuracy: 0.8691\n",
            "Epoch 5/100\n",
            "18/18 - 2s - loss: 0.2078 - accuracy: 0.9253 - val_loss: 0.3380 - val_accuracy: 0.8692\n",
            "Epoch 6/100\n",
            "18/18 - 2s - loss: 0.1678 - accuracy: 0.9441 - val_loss: 0.3573 - val_accuracy: 0.8672\n",
            "Epoch 7/100\n",
            "18/18 - 2s - loss: 0.1427 - accuracy: 0.9559 - val_loss: 0.3914 - val_accuracy: 0.8635\n",
            "Epoch 8/100\n",
            "18/18 - 2s - loss: 0.1223 - accuracy: 0.9637 - val_loss: 0.4227 - val_accuracy: 0.8575\n",
            "Epoch 9/100\n",
            "18/18 - 2s - loss: 0.1034 - accuracy: 0.9721 - val_loss: 0.4189 - val_accuracy: 0.8555\n",
            "Epoch 10/100\n",
            "18/18 - 2s - loss: 0.0914 - accuracy: 0.9753 - val_loss: 0.5049 - val_accuracy: 0.8495\n",
            "Epoch 11/100\n",
            "18/18 - 2s - loss: 0.0873 - accuracy: 0.9750 - val_loss: 0.4452 - val_accuracy: 0.8517\n",
            "Epoch 12/100\n",
            "18/18 - 2s - loss: 0.0836 - accuracy: 0.9754 - val_loss: 0.5232 - val_accuracy: 0.8504\n",
            "Epoch 13/100\n",
            "18/18 - 2s - loss: 0.0663 - accuracy: 0.9823 - val_loss: 0.5066 - val_accuracy: 0.8469\n",
            "Epoch 14/100\n",
            "18/18 - 2s - loss: 0.0503 - accuracy: 0.9889 - val_loss: 0.6059 - val_accuracy: 0.8437\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFCiusb_7gel",
        "outputId": "c5c0c856-091b-4d4e-cc7b-1b746aa1fc8c"
      },
      "source": [
        "y_pred = model.predict_classes(X_test)\n",
        "print(metrics.classification_report(y_test, y_pred))\n",
        "print(metrics.confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.88      0.87      3738\n",
            "           1       0.88      0.86      0.87      3762\n",
            "\n",
            "    accuracy                           0.87      7500\n",
            "   macro avg       0.87      0.87      0.87      7500\n",
            "weighted avg       0.87      0.87      0.87      7500\n",
            "\n",
            "[[3297  441]\n",
            " [ 541 3221]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROBFfB7D6Azw"
      },
      "source": [
        "Adding Dropout Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kApwztK58SS"
      },
      "source": [
        "\n",
        "## Creating model\n",
        "embedding_vector_features=100\n",
        "dropout_model=Sequential()\n",
        "dropout_model.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\n",
        "dropout_model.add(Dropout(0.3))\n",
        "dropout_model.add(LSTM(100))\n",
        "dropout_model.add(Dropout(0.3))\n",
        "dropout_model.add(Dense(1,activation='sigmoid'))\n",
        "dropout_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32rIQEOz6IXy",
        "outputId": "3cd8b27d-60b6-453c-e246-c7ed84ee024a"
      },
      "source": [
        "dropout = dropout_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    batch_size=256,\n",
        "    epochs=100,\n",
        "    callbacks=[early_stopping], # put your callbacks in a list\n",
        "    verbose=2,  # turn off training log\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "69/69 - 5s - loss: 0.5608 - accuracy: 0.7063 - val_loss: 0.4109 - val_accuracy: 0.8115\n",
            "Epoch 2/100\n",
            "69/69 - 3s - loss: 0.3147 - accuracy: 0.8740 - val_loss: 0.3099 - val_accuracy: 0.8684\n",
            "Epoch 3/100\n",
            "69/69 - 3s - loss: 0.2177 - accuracy: 0.9190 - val_loss: 0.3166 - val_accuracy: 0.8723\n",
            "Epoch 4/100\n",
            "69/69 - 3s - loss: 0.1725 - accuracy: 0.9377 - val_loss: 0.3396 - val_accuracy: 0.8608\n",
            "Epoch 5/100\n",
            "69/69 - 3s - loss: 0.1382 - accuracy: 0.9534 - val_loss: 0.4378 - val_accuracy: 0.8576\n",
            "Epoch 6/100\n",
            "69/69 - 3s - loss: 0.1211 - accuracy: 0.9572 - val_loss: 0.3997 - val_accuracy: 0.8539\n",
            "Epoch 7/100\n",
            "69/69 - 3s - loss: 0.1049 - accuracy: 0.9648 - val_loss: 0.4514 - val_accuracy: 0.8511\n",
            "Epoch 8/100\n",
            "69/69 - 3s - loss: 0.0847 - accuracy: 0.9722 - val_loss: 0.5127 - val_accuracy: 0.8519\n",
            "Epoch 9/100\n",
            "69/69 - 3s - loss: 0.0673 - accuracy: 0.9774 - val_loss: 0.5792 - val_accuracy: 0.8449\n",
            "Epoch 10/100\n",
            "69/69 - 3s - loss: 0.0665 - accuracy: 0.9782 - val_loss: 0.5331 - val_accuracy: 0.8460\n",
            "Epoch 11/100\n",
            "69/69 - 3s - loss: 0.0600 - accuracy: 0.9814 - val_loss: 0.6049 - val_accuracy: 0.8469\n",
            "Epoch 12/100\n",
            "69/69 - 3s - loss: 0.0489 - accuracy: 0.9850 - val_loss: 0.6158 - val_accuracy: 0.8460\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vIIOsBc6xMz",
        "outputId": "93b89b61-fc69-46ca-de24-7608e753c321"
      },
      "source": [
        "y_pred_dropout = dropout_model.predict_classes(X_test)\n",
        "y_pred_dropout"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [1],\n",
              "       [0],\n",
              "       ...,\n",
              "       [0],\n",
              "       [0],\n",
              "       [0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJBu56Kc6416",
        "outputId": "8c47fcc1-881e-4f24-eb6d-a524c6b271e2"
      },
      "source": [
        "print(metrics.classification_report(y_test, y_pred_dropout))\n",
        "print(metrics.confusion_matrix(y_test, y_pred_dropout))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.86      0.87      3738\n",
            "           1       0.86      0.87      0.87      3762\n",
            "\n",
            "    accuracy                           0.87      7500\n",
            "   macro avg       0.87      0.87      0.87      7500\n",
            "weighted avg       0.87      0.87      0.87      7500\n",
            "\n",
            "[[3224  514]\n",
            " [ 473 3289]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrS0pAS9mTXE"
      },
      "source": [
        "\n",
        "## Using Word2Vec model trained on unlabeled data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRCCN0Mtdsae"
      },
      "source": [
        "### Vector Averaging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJZrly4pdiNo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qZ9MIMr_3r_"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "import gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsX6TtRB_3xG"
      },
      "source": [
        "w2v = gensim.models.Word2Vec.load(\"/content/drive/MyDrive/Project data/Movie review sentiment analysis/Word2Vec_model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jwnudW6SHpC"
      },
      "source": [
        "def makeFeatureVec(words, model, num_features):\n",
        "    # Function to average all of the word vectors in a given\n",
        "    # paragraph\n",
        "    #\n",
        "    # Pre-initialize an empty numpy array (for speed)\n",
        "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
        "    #\n",
        "    nwords = 0\n",
        "    # \n",
        "    # Index2word is a list that contains the names of the words in \n",
        "    # the model's vocabulary. Convert it to a set, for speed \n",
        "    index2word_set = set(model.wv.index2word)\n",
        "    #\n",
        "    # Loop over each word in the review and, if it is in the model's\n",
        "    # vocaublary, add its feature vector to the total\n",
        "    for word in words:\n",
        "        if word in index2word_set: \n",
        "            nwords = nwords + 1\n",
        "            featureVec = np.add(featureVec,model[word])\n",
        "    # \n",
        "    # Divide the result by the number of words to get the average\n",
        "    featureVec = np.divide(featureVec,nwords)\n",
        "    return featureVec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kninjEYPSHrD"
      },
      "source": [
        "def getAvgFeatureVecs(reviews, model, num_features):\n",
        "    # Given a set of reviews (each one a list of words), calculate \n",
        "    # the average feature vector for each one and return a 2D numpy array \n",
        "    # \n",
        "    # Initialize a counter\n",
        "    counter = 0\n",
        "    # \n",
        "    # Preallocate a 2D numpy array, for speed\n",
        "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
        "    # \n",
        "    # Loop through the reviews\n",
        "    for review in reviews:\n",
        "       #\n",
        "       # Print a status message every 1000th review\n",
        "       if counter%1000 == 0:\n",
        "           print (\"Review %d of %d\" % (counter, len(reviews)))\n",
        "       # \n",
        "       # Call the function (defined above) that makes average feature vectors\n",
        "       reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
        "       #\n",
        "       # Increment the counter\n",
        "       counter = counter + 1\n",
        "    return reviewFeatureVecs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAxBdJkLSHxI"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(train['review'], train['sentiment'], test_size=0.30, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDWe1gFsSHyx"
      },
      "source": [
        "train_reviews = []\n",
        "for review in X_train:\n",
        "    train_reviews.append(preprocessor( review))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frrBEGjFSH1E"
      },
      "source": [
        "test_reviews = []\n",
        "for review in X_test:\n",
        "    test_reviews.append(preprocessor( review))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjE5Vd03ck2v",
        "outputId": "1b7038ef-6d50-4695-d586-30c0ef626b19"
      },
      "source": [
        "X_train = getAvgFeatureVecs( train_reviews, w2v, 300)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review 0 of 17500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Review 1000 of 17500\n",
            "Review 2000 of 17500\n",
            "Review 3000 of 17500\n",
            "Review 4000 of 17500\n",
            "Review 5000 of 17500\n",
            "Review 6000 of 17500\n",
            "Review 7000 of 17500\n",
            "Review 8000 of 17500\n",
            "Review 9000 of 17500\n",
            "Review 10000 of 17500\n",
            "Review 11000 of 17500\n",
            "Review 12000 of 17500\n",
            "Review 13000 of 17500\n",
            "Review 14000 of 17500\n",
            "Review 15000 of 17500\n",
            "Review 16000 of 17500\n",
            "Review 17000 of 17500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6zHqRsyck49",
        "outputId": "b4bfe4c5-47f1-4eb3-aea6-ca9b1a0c9ce1"
      },
      "source": [
        "X_test = getAvgFeatureVecs( test_reviews, w2v, 300)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review 0 of 7500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Review 1000 of 7500\n",
            "Review 2000 of 7500\n",
            "Review 3000 of 7500\n",
            "Review 4000 of 7500\n",
            "Review 5000 of 7500\n",
            "Review 6000 of 7500\n",
            "Review 7000 of 7500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kT_0HXZBc71V"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "forest = RandomForestClassifier( n_estimators = 100 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "175CXTbjc73l",
        "outputId": "2c6ae6fa-9908-4625-b69a-7250fb1dad26"
      },
      "source": [
        "forest.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzuQgmSkc79R",
        "outputId": "f6e878d1-68a8-4aa3-c295-927a1391f9f6"
      },
      "source": [
        "y_pred = forest.predict(X_test)\n",
        "from sklearn import metrics\n",
        "print(metrics.classification_report(y_test, y_pred))\n",
        "print(metrics.confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.59      0.59      3738\n",
            "           1       0.59      0.59      0.59      3762\n",
            "\n",
            "    accuracy                           0.59      7500\n",
            "   macro avg       0.59      0.59      0.59      7500\n",
            "weighted avg       0.59      0.59      0.59      7500\n",
            "\n",
            "[[2195 1543]\n",
            " [1535 2227]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CKZRNqEck84"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}